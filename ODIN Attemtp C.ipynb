{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23fac96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "import fiftyone.utils.torch as fotorch\n",
    "import fiftyone.utils.annotations as foua\n",
    "import fiftyone.utils.patches as foup\n",
    "\n",
    "import torch\n",
    "import torchvision.datasets as dset\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torchvision as tv\n",
    "from torchvision import transforms as tf\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision.transforms import functional as tfunc\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import torchviz\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans, OPTICS, DBSCAN, Birch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fe8916",
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_2017_DATASET_DIR = Path(\"./data/coco_2017/\")\n",
    "def make_dataloaders():\n",
    "    if not COCO_2017_DATASET_DIR.exists():\n",
    "        foz.download_zoo_dataset(\"coco-2017\", dataset_dir=str(COCO_2017_DATASET_DIR))\n",
    "    \n",
    "    training_dataset = dset.CocoDetection(\n",
    "        root=str(COCO_2017_DATASET_DIR.joinpath(\"train/data\")),\n",
    "        annFile=str(COCO_2017_DATASET_DIR.joinpath(\"train/labels.json\")),\n",
    "        transform=tf.Compose([tf.ToTensor()])\n",
    "        )\n",
    "    training_dataloader = DataLoader(training_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "    testing_dataset = dset.CocoDetection(\n",
    "        root=str(COCO_2017_DATASET_DIR.joinpath(\"test/data\")),\n",
    "        annFile=str(COCO_2017_DATASET_DIR.joinpath(\"test/labels.json\")),\n",
    "        transform=tf.Compose([tf.ToTensor()])\n",
    "        )\n",
    "    testing_dataloader = DataLoader(testing_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "    validation_dataset = dset.CocoDetection(\n",
    "        root=str(COCO_2017_DATASET_DIR.joinpath(\"validation/data\")),\n",
    "        annFile=str(COCO_2017_DATASET_DIR.joinpath(\"train/labels.json\")),\n",
    "        transform=tf.Compose([tf.ToTensor()])\n",
    "        )\n",
    "    validation_dataloader = DataLoader(validation_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "    return training_dataloader, testing_dataloader, validation_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231c98d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_image(image):\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        if str(image.device) == 'cuda:0':\n",
    "            image = image.detach().cpu()\n",
    "        image = image.squeeze().numpy()\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.axis(\"off\")\n",
    "    if len(image.shape) == 2:\n",
    "        plt.imshow(image, interpolation=\"none\")\n",
    "    else:\n",
    "        image = np.transpose(image, (1,2,0))\n",
    "        plt.imshow(image, interpolation=\"none\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "def draw_layers(data):\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        if str(data.device) == 'cuda:0':\n",
    "            data = data.detach().cpu()\n",
    "        data = data.squeeze().numpy()\n",
    "    if len(data.shape) == 2:\n",
    "        return draw_image(data)\n",
    "    data_layers = [tfunc.to_tensor(d) for d in data]\n",
    "    grid_image = vutils.make_grid(data_layers, nrow=int(len(data_layers)**0.5), padding=0, pad_value=0.5, normalize=True).cpu()\n",
    "    draw_image(grid_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1811a436",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FPNStage(nn.Module):\n",
    "    def __init__(self, fpn_dim, bbone_dim):\n",
    "        super().__init__()\n",
    "        self.lat = nn.Conv2d(bbone_dim, fpn_dim, kernel_size=1)\n",
    "        self.top = nn.ConvTranspose2d(fpn_dim, fpn_dim, kernel_size=4, stride=2, padding=1)\n",
    "        self.aa  = nn.Conv2d(fpn_dim, fpn_dim, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "    def forward(self, bbone_activation, prev_fpn_stage):\n",
    "        lat_out = self.lat(bbone_activation)\n",
    "        top_out = self.top(prev_fpn_stage)\n",
    "        \n",
    "        if not lat_out.shape == top_out.shape:\n",
    "            top_out = nn.UpsamplingNearest2d(size=lat_out.shape[2:])(top_out)          \n",
    "            \n",
    "        final_out = self.aa(lat_out + top_out)\n",
    "        return final_out\n",
    "        \n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, fpn_dim=256):\n",
    "        super().__init__()\n",
    "        self.num_fpn_stages = 4\n",
    "        self.fpn_dim = fpn_dim\n",
    "        \n",
    "        self.activation = {}\n",
    "        self.resnet50_backbone = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        self.resnet50_backbone.layer1.register_forward_hook(self.get_activation('conv2'))\n",
    "        self.resnet50_backbone.layer2.register_forward_hook(self.get_activation('conv3'))\n",
    "        self.resnet50_backbone.layer3.register_forward_hook(self.get_activation('conv4'))\n",
    "        self.resnet50_backbone.layer4.register_forward_hook(self.get_activation('conv5'))\n",
    "        \n",
    "        self.fpn_stage_1 = nn.Conv2d(2048, self.fpn_dim, kernel_size=1)\n",
    "        self.fpn_stage_2 = FPNStage(self.fpn_dim, 1024)\n",
    "        self.fpn_stage_3 = FPNStage(self.fpn_dim, 512)\n",
    "        self.fpn_stage_4 = FPNStage(self.fpn_dim, 256)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        backbone_output = self.resnet50_backbone(input)\n",
    "        fpn_stage_1_output = self.fpn_stage_1(self.activation['conv5'])\n",
    "        fpn_stage_2_output = self.fpn_stage_2(self.activation['conv4'], fpn_stage_1_output)\n",
    "        fpn_stage_3_output = self.fpn_stage_3(self.activation['conv3'], fpn_stage_2_output)\n",
    "        fpn_stage_4_output = self.fpn_stage_4(self.activation['conv2'], fpn_stage_3_output)\n",
    "\n",
    "        \n",
    "        return fpn_stage_4_output\n",
    "        \n",
    "    def get_activation(self, name):\n",
    "        def hook(model, input, output):\n",
    "            self.activation[name] = output.detach()\n",
    "        return hook \n",
    "\n",
    "\n",
    "class Projector(nn.Module):\n",
    "    def __init__(self, fpn_dim=256):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(fpn_dim, fpn_dim, 1),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Linear(fpn_dim, fpn_dim, 1),\n",
    "            nn.ReLU(inplace=False)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "\n",
    "class Predictor(nn.Module):\n",
    "    def __init__(self, fpn_dim=256):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(fpn_dim, fpn_dim, 1),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Linear(fpn_dim, fpn_dim, 1),\n",
    "            nn.ReLU(inplace=False)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "\n",
    "class TauModel(nn.Module):\n",
    "    def __init__(self, fpn_dim=256):\n",
    "        super().__init__()\n",
    "        self.f = FeatureExtractor(fpn_dim=fpn_dim)\n",
    "        self.g = Projector(fpn_dim=fpn_dim)\n",
    "        self.q = Predictor(fpn_dim=fpn_dim)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        h = tfunc.resize(self.f(input), (448, 448), tf.InterpolationMode.BILINEAR)\n",
    "        z = self.g(h.transpose(1,3))\n",
    "        p = self.q(z)\n",
    "        return h, z, p\n",
    "        \n",
    "class ThetaXiModel(nn.Module):\n",
    "    def __init__(self, fpn_dim=256):\n",
    "        super().__init__()\n",
    "        self.f = FeatureExtractor(fpn_dim=fpn_dim)\n",
    "        self.g = Projector(fpn_dim=fpn_dim)\n",
    "        self.q = Predictor(fpn_dim=fpn_dim)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        h = tfunc.resize(self.f(input), (224, 224), tf.InterpolationMode.BILINEAR)\n",
    "        z = self.g(h)\n",
    "        p = self.q(z)\n",
    "        return h, z, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cb3a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViewGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "    nn.Module sub-class which, when called on an image, generates three views of the image, v0, v1, and v2. v1\n",
    "    and v2 are generated first, and then v0 is generated from their bounding box. The class also has a\n",
    "    `reverse` method which, given an image the size of v0, produces the equivalent v1 and v2 crops from it.\n",
    "    \"\"\"\n",
    "    SCALE_RANGE = (0.08, 1.0)\n",
    "    RATIO_RANGE = (0.75, 1.33333333333)\n",
    "\n",
    "    FLIP_PROB = 0.5\n",
    "\n",
    "    COLOR_JITTER_PROB = 0.8\n",
    "    COLOR_OPERATIONS = [\"brightness\", \"contrast\", \"saturation\", \"hue\"]\n",
    "    BRIGHTNESS_MAX = 0.4\n",
    "    CONTRAST_MAX = 0.4\n",
    "    SATURATION_MAX = 0.2\n",
    "    HUE_MAX = 0.1\n",
    "\n",
    "    GRAY_PROB = 0.2\n",
    "\n",
    "    v1_BLUR_PROB = 1.0\n",
    "    v2_BLUR_PROB = 0.1\n",
    "\n",
    "    v1_SOLAR_PROB = 0.0\n",
    "    v2_SOLAR_PROB = 0.2\n",
    "\n",
    "    v0_SHAPE = (448, 448)\n",
    "    v1_SHAPE = v2_SHAPE = (224, 224)\n",
    "    INTERPOLATION = tf.InterpolationMode.BILINEAR\n",
    "\n",
    "    def __init__(self, image):\n",
    "        super().__init__()\n",
    "        self.crop_v1= tf.RandomResizedCrop.get_params(image, self.SCALE_RANGE, self.RATIO_RANGE)\n",
    "        self.crop_v2 = tf.RandomResizedCrop.get_params(image, self.SCALE_RANGE, self.RATIO_RANGE)\n",
    "        \n",
    "        flip_v1 = random.random() < self.FLIP_PROB\n",
    "        flip_v2 = random.random() < self.FLIP_PROB\n",
    "        \n",
    "        gray_v1 = random.random() < self.GRAY_PROB\n",
    "        gray_v2 = random.random() < self.GRAY_PROB\n",
    "        \n",
    "        blur_v1 = random.random() < self.v1_BLUR_PROB\n",
    "        blur_v2 = random.random() < self.v2_BLUR_PROB\n",
    "        \n",
    "        solar_v1 = random.random() < self.v1_SOLAR_PROB\n",
    "        solar_v2 = random.random() < self.v2_SOLAR_PROB\n",
    "        \n",
    "        if random.random() < self.COLOR_JITTER_PROB: \n",
    "            color_params = tf.ColorJitter.get_params(\n",
    "                [max(0, 1 - self.BRIGHTNESS_MAX), 1 + self.BRIGHTNESS_MAX],\n",
    "                [max(0, 1 - self.CONTRAST_MAX), 1 + self.CONTRAST_MAX],\n",
    "                [max(0, 1 - self.SATURATION_MAX), 1 + self.SATURATION_MAX],\n",
    "                [-self.HUE_MAX, self.HUE_MAX]\n",
    "            )\n",
    "            order = color_params[0]\n",
    "            color_params = color_params[1:]\n",
    "            jitter_v1 = [(self.COLOR_OPERATIONS[i], color_params[i]) for i in order]\n",
    "        else:\n",
    "            jitter_v1 = None\n",
    "        if random.random() < self.COLOR_JITTER_PROB: \n",
    "            color_params = tf.ColorJitter.get_params(\n",
    "                [max(0, 1 - self.BRIGHTNESS_MAX), 1 + self.BRIGHTNESS_MAX],\n",
    "                [max(0, 1 - self.CONTRAST_MAX), 1 + self.CONTRAST_MAX],\n",
    "                [max(0, 1 - self.SATURATION_MAX), 1 + self.SATURATION_MAX],\n",
    "                [-self.HUE_MAX, self.HUE_MAX]\n",
    "            )\n",
    "            order = color_params[0]\n",
    "            color_params = color_params[1:]\n",
    "            jitter_v2 = [(self.COLOR_OPERATIONS[i], color_params[i]) for i in order]\n",
    "        else:\n",
    "            jitter_v2 = None\n",
    "        \n",
    "        self.v1_params = (self.crop_v1, flip_v1, jitter_v1, gray_v1, blur_v1, solar_v1, self.v1_SHAPE)\n",
    "        self.v2_params = (self.crop_v2, flip_v2, jitter_v2, gray_v2, blur_v2, solar_v2, self.v2_SHAPE)\n",
    "        \n",
    "        self.v1_proportional_crop = None\n",
    "        self.v2_proportional_crop = None\n",
    "        \n",
    "    def __call__(self, img):\n",
    "        v1 = self._generate_sub_view(img, self.v1_params)\n",
    "        v2 = self._generate_sub_view(img, self.v2_params)\n",
    "        v0 = self._generate_whole_view(img, self.crop_v1, self.crop_v2)\n",
    "        \n",
    "        return v0, v1, v2\n",
    "    \n",
    "    def reverse(self, image):\n",
    "        image_height, image_width = image.shape[-2:]\n",
    "        \n",
    "        v1_top_scaled = round(self.v1_proportional_crop[0] * image_height)\n",
    "        v1_left_scaled = round(self.v1_proportional_crop[1] * image_width)\n",
    "        v1_height_scaled = round(self.v1_proportional_crop[2] * image_height)\n",
    "        v1_width_scaled = round(self.v1_proportional_crop[3] * image_width)\n",
    "        \n",
    "        v2_top_scaled = round(self.v2_proportional_crop[0] * image_height)\n",
    "        v2_left_scaled = round(self.v2_proportional_crop[1] * image_width)\n",
    "        v2_height_scaled = round(self.v2_proportional_crop[2] * image_height)\n",
    "        v2_width_scaled = round(self.v2_proportional_crop[3] * image_width)\n",
    "        \n",
    "        v1_scaled = tfunc.resized_crop(image, v1_top_scaled, v1_left_scaled, v1_height_scaled, v1_width_scaled, self.v1_SHAPE, tf.InterpolationMode.BILINEAR)\n",
    "        v2_scaled = tfunc.resized_crop(image, v2_top_scaled, v2_left_scaled, v2_height_scaled, v2_width_scaled, self.v2_SHAPE, tf.InterpolationMode.BILINEAR)\n",
    "        \n",
    "        if self.v1_params[1]: v1_scaled = tfunc.hflip(v1_scaled)\n",
    "        if self.v2_params[1]: v2_scaled = tfunc.hflip(v2_scaled)\n",
    "        \n",
    "        return v1_scaled, v2_scaled\n",
    "        \n",
    "    def _generate_whole_view(self, image, crop_v1, crop_v2):\n",
    "        v1_top, v1_left, v1_height, v1_width = crop_v1\n",
    "        v2_top, v2_left, v2_height, v2_width = crop_v2\n",
    "        v1_bot = v1_top + v1_height\n",
    "        v1_right = v1_left + v1_width\n",
    "        v2_bot = v2_top + v2_height\n",
    "        v2_right = v2_left + v2_width\n",
    "        \n",
    "        v0_top = min(v1_top, v2_top)\n",
    "        v0_left = min(v1_left, v2_left)\n",
    "        v0_bot = max(v1_bot, v2_bot)    \n",
    "        v0_right = max(v1_right, v2_right)\n",
    "        v0_height = v0_bot - v0_top\n",
    "        v0_width = v0_right - v0_left\n",
    "        \n",
    "        self.v0_crop = (v0_top, v0_left, v0_height, v0_width)\n",
    "        \n",
    "        v1_proportional_top = (v1_top - v0_top)/v0_height\n",
    "        v1_proportional_left = (v1_left - v0_left)/v0_width\n",
    "        v1_proportional_height = v1_height/v0_height\n",
    "        v1_proportional_width = v1_width/v0_width\n",
    "        self.v1_proportional_crop = (v1_proportional_top, v1_proportional_left, v1_proportional_height, v1_proportional_width)\n",
    "        \n",
    "        v2_proportional_top = (v2_top - v0_top)/v0_height\n",
    "        v2_proportional_left = (v2_left - v0_left)/v0_width\n",
    "        v2_proportional_height = v2_height/v0_height\n",
    "        v2_proportional_width = v2_width/v0_width\n",
    "        self.v2_proportional_crop = (v2_proportional_top, v2_proportional_left, v2_proportional_height, v2_proportional_width)\n",
    "        \n",
    "        return tfunc.resized_crop(image, v0_top, v0_left, v0_height, v0_width, self.v0_SHAPE, tf.InterpolationMode.BILINEAR)\n",
    "    \n",
    "    def _generate_sub_view(self, image, params):\n",
    "        crop, flip, jitter, gray, blur, solar, shape = params\n",
    "        \n",
    "        t, l, h, w = crop\n",
    "        output = tfunc.resized_crop(image, t, l, h, w, shape, self.INTERPOLATION)\n",
    "        if flip: output = tfunc.hflip(output)\n",
    "        if jitter is not None:\n",
    "            for param, value in jitter:\n",
    "                if param == \"brightness\": output = tfunc.adjust_brightness(output, value)\n",
    "                elif param == \"contrast\": output = tfunc.adjust_contrast(output, value)\n",
    "                elif param == \"hue\": output = tfunc.adjust_hue(output, value)\n",
    "                elif param == \"saturation\": output = tfunc.adjust_saturation(output, value)\n",
    "        if gray: output = tfunc.rgb_to_grayscale(output, 3)\n",
    "        if blur: output = tfunc.gaussian_blur(output, 23, (0.1, 2.0))\n",
    "        if solar: output = tfunc.solarize(output, 0.5)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b11962",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, test_dataloader, val_dataloader = make_dataloaders()\n",
    "fpn_dim=64\n",
    "lr_tau = 1e-3\n",
    "lr_theta = 1e-2\n",
    "lr_xi = 1e-3\n",
    "device = 'cuda:0'\n",
    "num_clusters = 8\n",
    "\n",
    "f_tau = FeatureExtractor(fpn_dim).to(device)\n",
    "f_theta = FeatureExtractor(fpn_dim).to(device)\n",
    "f_xi = FeatureExtractor(fpn_dim).to(device)\n",
    "\n",
    "g_tau = Projector(fpn_dim).to(device)\n",
    "g_theta = Projector(fpn_dim).to(device)\n",
    "g_xi = Projector(fpn_dim).to(device)\n",
    "\n",
    "q_theta = Predictor(fpn_dim).to(device)\n",
    "\n",
    "f_theta_optim = optim.SGD(f_theta.parameters(), lr=lr_theta)\n",
    "g_theta_optim = optim.SGD(g_theta.parameters(), lr=lr_theta)\n",
    "q_theta_optim = optim.SGD(q_theta.parameters(), lr=lr_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24400852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusterer(h0, z0, eps_coeff=1.0, num_clusters=4, clusterer_type=\"kmeans\"):\n",
    "    z0_dists = torch.cdist(z0, z0, p=2)\n",
    "    z0_dist_mean = z0_dists.mean().item()\n",
    "    z0_dist_std_dev = z0_dists.std().item()\n",
    "    z0_dist_median = z0_dists.median().item()\n",
    "\n",
    "    z0_norms = torch.norm(z0, p=2, dim=1)\n",
    "    z0_norms_mean = z0_norms.mean().item()\n",
    "    z0_norms_std_dev = z0_norms.std().item()\n",
    "    z0_norms_median = z0_norms.median().item()\n",
    "\n",
    "    epsilon = z0_norms_median * eps_coeff\n",
    "\n",
    "    clusterer_OPTICS = OPTICS(\n",
    "        cluster_method=\"dbscan\",\n",
    "        min_samples=0.05, \n",
    "        eps=epsilon,\n",
    "        n_jobs=4\n",
    "        )\n",
    "    clusterer_kmeans = KMeans(\n",
    "        n_clusters=num_clusters,\n",
    "        n_init=10, \n",
    "        max_iter=500,\n",
    "        tol=0.0001,\n",
    "        copy_x=False,\n",
    "        algorithm='elkan'\n",
    "        )\n",
    "\n",
    "    if clusterer_type == \"kmeans\":\n",
    "        return clusterer_kmeans\n",
    "    elif clusterer_type == \"optics\":\n",
    "        return clusterer_OPTICS\n",
    "    elif clusterer_type == \"both\":\n",
    "        return clusterer_kmeans, clusterer_OPTICS\n",
    "    else:\n",
    "        raise ValueError(clusterer_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4461399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_masks(feature_map, clusterer, view_gen=None):\n",
    "    feature_map_np = np.transpose(feature_map.detach().cpu().squeeze(),(1,2,0))\n",
    "\n",
    "    original_shape = feature_map_np.shape\n",
    "    flat_shape = (original_shape[0]*original_shape[1], original_shape[2])\n",
    "    feature_map_np_flat = feature_map_np.reshape((flat_shape))\n",
    "\n",
    "    mask_assignments_flat = clusterer.fit_predict(feature_map_np_flat)\n",
    "    cluster_ids = set(mask_assignments_flat)\n",
    "    if -1 in cluster_ids:\n",
    "        cluster_ids.remove(-1)\n",
    "\n",
    "    mask_assignments = tfunc.to_tensor(mask_assignments_flat.reshape(original_shape[:2])).squeeze()\n",
    "    m0_layers = [torch.where(mask_assignments==c_id, 1., 0.).numpy() for c_id in sorted(cluster_ids)]\n",
    "    m0_np = np.stack(m0_layers, 2)\n",
    "    m0 = tfunc.to_tensor(m0_np).to(device)\n",
    "\n",
    "    if view_gen:    \n",
    "        m1_raw, m2_raw = view_gen.reverse(m0)\n",
    "        m1 = m1_raw[torch.argwhere(m1_raw.sum(dim=(1,2))>0)].squeeze()\n",
    "        m2 = m2_raw[torch.argwhere(m2_raw.sum(dim=(1,2))>0)].squeeze()\n",
    "    else:\n",
    "        m1 = m2 = None\n",
    "        \n",
    "    m0 = m0.reshape((1,) + m0.shape)\n",
    "    m1 = m1.reshape((1,) + m1.shape)\n",
    "    m2 = m2.reshape((1,) + m2.shape)\n",
    "    return cluster_ids, mask_assignments, m0, m1, m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7455603f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tau_network(v0):\n",
    "    h_0 = f_tau(v0)\n",
    "    z_0 = g_tau(h_0.transpose(1,3)).transpose(3,1)\n",
    "    return h_0, z_0\n",
    "\n",
    "def run_theta_network(v1, v2, m1, m2):\n",
    "    h_1 = tfunc.resize(f_theta(v1), (224,224), tf.InterpolationMode.BILINEAR)\n",
    "    h_2 = tfunc.resize(f_theta(v2), (224,224), tf.InterpolationMode.BILINEAR)\n",
    "\n",
    "    masked_h_1 = torch.concat([torch.concat([torch.mul(m1_i, h_1) for m1_i in m1_j]) for m1_j in m1])\n",
    "    masked_h_2 = torch.concat([torch.concat([torch.mul(m2_i, h_1) for m2_i in m2_j]) for m2_j in m2])\n",
    "\n",
    "    sum_masked_h_1 = masked_h_1.sum(dim=(2,3))\n",
    "    sum_masked_h_2 = masked_h_2.sum(dim=(2,3))\n",
    "\n",
    "    m1_sums = m1.sum(dim=(2,3)).transpose(1,0)\n",
    "    m2_sums = m2.sum(dim=(2,3)).transpose(1,0)\n",
    "\n",
    "    h_k_1 = sum_masked_h_1/m1_sums\n",
    "    h_k_2 = sum_masked_h_2/m2_sums\n",
    "\n",
    "    z_k_1 = g_theta(h_k_1)\n",
    "    z_k_2 = g_theta(h_k_2)\n",
    "\n",
    "    p_k_1 = q_theta(z_k_1)\n",
    "    p_k_2 = q_theta(z_k_2)\n",
    "\n",
    "    return (h_1, h_2), (masked_h_1, masked_h_2), (h_k_1, h_k_2), (z_k_1, z_k_2), (p_k_1, p_k_2)\n",
    "\n",
    "def run_xi_network(v1, v2, m1, m2):\n",
    "    h_1 = tfunc.resize(self.f_xi(v1), (224,224), tf.InterpolationMode.BILINEAR)\n",
    "    h_2 = tfunc.resize(self.f_xi(v2), (224,224), tf.InterpolationMode.BILINEAR)\n",
    "\n",
    "    h_k_1 = compute_mask_pooled_vectors(h_1, m1)\n",
    "    h_k_2 = compute_mask_pooled_vectors(h_2, m2)\n",
    "\n",
    "    z_k_1 = self.g_xi(h_k_1)\n",
    "    z_k_2 = self.g_xi(h_k_2)\n",
    "\n",
    "    return (h_1, h_2), (h_k_1, h_k_2), (z_k_1, z_k_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82367c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_mask_similarity(p_theta, z_xi, alpha):\n",
    "    top = torch.dot(p_theta, z_xi)\n",
    "    bot = torch.mul(torch.norm(p_theta), torch.norm(z_xi))\n",
    "    return (1.0/alpha)*(top/bot)\n",
    "\n",
    "def feature_contrastive_loss(p_k_1_theta, z_k_2_xi, index, alpha=0.1):\n",
    "    if index >= len(p_k_1_theta): return 0\n",
    "    if index >= len(z_k_2_xi): return 0\n",
    "    positive_similarity = single_mask_similarity(p_k_1_theta[index], z_k_2_xi[index], alpha)\n",
    "    negative_similarities_sum = sum([single_mask_similarity(p_k_1_theta[index], zk2_xi, alpha) for i, zk2_xi in enumerate(z_k_2_xi) if i != index])\n",
    "    bot = positive_similarity + negative_similarities_sum\n",
    "    nll = -1*torch.log(positive_similarity/bot)\n",
    "    return nll\n",
    "\n",
    "def total_contrastive_loss(p_k_1_theta, p_k_2_theta, z_k_1_xi, z_k_2_xi, alpha):\n",
    "    cum_loss = torch.Tensor([0.]).to(device)\n",
    "    num_masks_found = min([len(p_k_1_theta), len(p_k_2_theta), len(z_k_1_xi), len(z_k_2_xi)])\n",
    "    if num_masks_found == 0:\n",
    "        num_masks_found = 1e-5\n",
    "    for mask_idx in range(num_masks_found):\n",
    "        l_12_k = feature_contrastive_loss(p_k_1_theta, z_k_2_xi, mask_idx, alpha)\n",
    "        l_21_k = feature_contrastive_loss(p_k_2_theta, z_k_1_xi, mask_idx, alpha)\n",
    "        cum_loss += l_12_k + l_21_k\n",
    "    return cum_loss/num_masks_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f7062d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def within_mask_closeness_loss(masked_h):\n",
    "    within_h_max = torch.where(masked_h != 0, masked_h, float(\"-Inf\")).amax(dim=(2,3))\n",
    "    within_h_min = torch.where(masked_h != 0, masked_h, float(\"Inf\")).amin(dim=(2,3))\n",
    "    within_h_range = within_h_max - within_h_min\n",
    "    average_range = within_h_range.sum()/within_h_range.shape[0]\n",
    "    return average_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d358b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def byol_parameter_adjustment(param_zip):\n",
    "    with torch.no_grad():\n",
    "        for p_f_tau, p_f_theta, p_f_xi in param_zip:\n",
    "            new_p_f_xi = (1-lr_xi)*p_f_xi + lr_xi*p_f_theta\n",
    "            new_p_f_tau = (1-lr_tau)*p_f_tau + lr_tau*p_f_theta\n",
    "\n",
    "            p_f_xi.copy_(new_p_f_xi)\n",
    "            p_f_tau.copy_(new_p_f_tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7666a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_networks_training_mode(v1, v2, m1, m2):\n",
    "    _, __, ___, (pk1_theta, pk2_theta) = run_theta_network(v1, v2, m1, m2)\n",
    "    _, __, (zk1_xi, zk2_xi) = run_xi_network(v1, v2, m1, m2)\n",
    "    loss = total_contrastive_loss(pk1_theta, pk2_theta, zk1_xi, zk2_xi, 0.1)\n",
    "\n",
    "    loss.backward()\n",
    "    f_theta_optim.step()\n",
    "    g_theta_optim.step()\n",
    "    q_theta_optim.step()\n",
    "\n",
    "    byol_parameter_adjustment(zip(f_tau.parameters(), f_theta.parameters(), f_xi.parameters()))\n",
    "    byol_parameter_adjustment(zip(g_tau.parameters(), g_theta.parameters(), g_xi.parameters()))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6622dd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_iteration(input_tensor, eps_coeff, clusterer_type=\"kmeans\"):\n",
    "    view_gen = ViewGenerator(input_tensor)\n",
    "    v0, v1, v2 = view_gen(input_tensor)\n",
    "\n",
    "    h0, z0 = run_tau_network(v0)\n",
    "    cluster_results = dict()\n",
    "\n",
    "    loss = torch.Tensor([0.]).to(device)\n",
    "    if clusterer_type in [\"kmeans\", \"both\"]:\n",
    "        clusterer = get_clusterer(h0, z0, eps_coeff, num_clusters, clusterer_type=\"kmeans\")\n",
    "        cluster_ids, masks, m0, m1, m2 = generate_masks(z0, view_gen, clusterer)\n",
    "        loss += run_networks_training_mode(v1,v2, m1, m2)\n",
    "        cluster_results[\"kmeans\"] = (cluster_ids, masks, m0, m1, m2)\n",
    "\n",
    "    if clusterer_type in [\"optics\", \"both\"]:\n",
    "        clusterer = get_clusterer(h0, z0, eps_coeff, num_clusters, clusterer_type=\"optics\")\n",
    "        cluster_ids, masks, m0, m1, m2 = generate_masks(z0, view_gen, clusterer)\n",
    "        loss += run_networks_training_mode(v1,v2, m1, m2)\n",
    "        cluster_results[\"optics\"] = (cluster_ids, masks, m0, m1, m2)\n",
    "\n",
    "    return cluster_results, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5bed01",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = next(iter(test_dataloader))[0].to(device)\n",
    "draw_image(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d20c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_gen = ViewGenerator(input_tensor)\n",
    "v0, v1, v2 = view_gen(input_tensor)\n",
    "\n",
    "h0, z0 = run_tau_network(v0)\n",
    "clusterer = get_clusterer(h0, z0, eps_coeff=1.0, num_clusters=num_clusters, clusterer_type=\"kmeans\")\n",
    "cluster_ids, mask_assignments, m0, m1, m2 = generate_masks(h0, clusterer, view_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff725b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_1_theta = tfunc.resize(f_theta(v1), (224,224), tf.InterpolationMode.BILINEAR)\n",
    "h_2_theta = tfunc.resize(f_theta(v2), (224,224), tf.InterpolationMode.BILINEAR)\n",
    "\n",
    "masked_h_1_theta = torch.concat([torch.concat([torch.mul(m1_i, h_1_theta) for m1_i in m1_j]) for m1_j in m1])\n",
    "masked_h_2_theta = torch.concat([torch.concat([torch.mul(m2_i, h_2_theta) for m2_i in m2_j]) for m2_j in m2])\n",
    "\n",
    "sum_masked_h_1_theta = masked_h_1_theta.sum(dim=(2,3))\n",
    "sum_masked_h_2_theta = masked_h_2_theta.sum(dim=(2,3))\n",
    "\n",
    "m1_sums_theta = m1.sum(dim=(2,3)).transpose(1,0)\n",
    "m2_sums_theta = m2.sum(dim=(2,3)).transpose(1,0)\n",
    "\n",
    "h_k_1_theta = sum_masked_h_1_theta/m1_sums_theta\n",
    "h_k_2_theta = sum_masked_h_2_theta/m2_sums_theta\n",
    "\n",
    "z_k_1_theta = g_theta(h_k_1_theta)\n",
    "z_k_2_theta = g_theta(h_k_2_theta)\n",
    "\n",
    "p_k_1_theta = q_theta(z_k_1_theta)\n",
    "p_k_2_theta = q_theta(z_k_2_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88615e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_1_xi = tfunc.resize(f_xi(v1), (224,224), tf.InterpolationMode.BILINEAR)\n",
    "h_2_xi = tfunc.resize(f_xi(v2), (224,224), tf.InterpolationMode.BILINEAR)\n",
    "\n",
    "masked_h_1_xi = torch.concat([torch.concat([torch.mul(m1_i, h_1_xi) for m1_i in m1_j]) for m1_j in m1])\n",
    "masked_h_2_xi = torch.concat([torch.concat([torch.mul(m2_i, h_2_xi) for m2_i in m2_j]) for m2_j in m2])\n",
    "\n",
    "sum_masked_h_1_xi = masked_h_1_xi.sum(dim=(2,3))\n",
    "sum_masked_h_2_xi = masked_h_2_xi.sum(dim=(2,3))\n",
    "\n",
    "m1_sums_xi = m1.sum(dim=(2,3)).transpose(1,0)\n",
    "m2_sums_xi = m2.sum(dim=(2,3)).transpose(1,0)\n",
    "\n",
    "h_k_1_xi = sum_masked_h_1_xi/m1_sums_xi\n",
    "h_k_2_xi = sum_masked_h_2_xi/m2_sums_xi\n",
    "\n",
    "z_k_1_xi = g_xi(h_k_1_xi)\n",
    "z_k_2_xi = g_xi(h_k_2_xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e433684e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
