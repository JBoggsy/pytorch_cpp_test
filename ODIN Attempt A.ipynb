{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1797223f",
   "metadata": {},
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2be280dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "import fiftyone.utils.torch as fotorch\n",
    "import fiftyone.utils.annotations as foua\n",
    "import fiftyone.utils.patches as foup\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torchvision as tv\n",
    "from torchvision import transforms as tf\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision.transforms import functional as tfunc\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import torchviz\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans, OPTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba461cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model on GPU if it is available\n",
    "ngpu = 1\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449b989a",
   "metadata": {},
   "source": [
    "#  Get COCO 2017 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "006c4530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'train' to '/home/boggsj/fiftyone/coco-2017/train' if necessary\n",
      "Found annotations at '/home/boggsj/fiftyone/coco-2017/raw/instances_train2017.json'\n",
      "Images already downloaded\n",
      "Existing download of split 'train' is sufficient\n",
      "Loading existing dataset 'detector-recipe'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n",
      "Downloading split 'test' to '/home/boggsj/fiftyone/coco-2017/test' if necessary\n",
      "Found test info at '/home/boggsj/fiftyone/coco-2017/raw/image_info_test2017.json'\n",
      "Images already downloaded\n",
      "Existing download of split 'test' is sufficient\n",
      "Loading existing dataset 'detector-recipe'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n",
      "Downloading split 'validation' to '/home/boggsj/fiftyone/coco-2017/validation' if necessary\n",
      "Found annotations at '/home/boggsj/fiftyone/coco-2017/raw/instances_val2017.json'\n",
      "Images already downloaded\n",
      "Existing download of split 'validation' is sufficient\n",
      "Loading existing dataset 'detector-recipe'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
     ]
    }
   ],
   "source": [
    "coco_17_train = foz.load_zoo_dataset(\n",
    "    \"coco-2017\",\n",
    "    split=\"train\",\n",
    "    dataset_name=\"detector-recipe\",\n",
    "    label_types=[\"detections\", \"segmentations\"]\n",
    ")\n",
    "\n",
    "coco_17_test = foz.load_zoo_dataset(\n",
    "    \"coco-2017\",\n",
    "    split=\"test\",\n",
    "    dataset_name=\"detector-recipe\",\n",
    "    label_types=[\"detections\", \"segmentations\"]\n",
    ")\n",
    "\n",
    "coco_17_validation = foz.load_zoo_dataset(\n",
    "    \"coco-2017\",\n",
    "    split=\"validation\",\n",
    "    dataset_name=\"detector-recipe\",\n",
    "    label_types=[\"segmentations\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a8a8b9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:5151/?context=ipython&subscription=23bb1167-977a-4312-91f7-c07caf500b49\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fbfb2557130>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "session = fo.launch_app(coco_17_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e72dcfd8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"No sample found with ID '6350559f099f10e8691afb1f'\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_input_raw \u001b[38;5;241m=\u001b[39m \u001b[43mcoco_17_validation\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m6350559f099f10e8691afb1f\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      2\u001b[0m test_image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(Image\u001b[38;5;241m.\u001b[39mopen(test_input_raw\u001b[38;5;241m.\u001b[39mfilepath))\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      3\u001b[0m test_tens \u001b[38;5;241m=\u001b[39m tfunc\u001b[38;5;241m.\u001b[39mto_tensor(test_image)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_test/lib/python3.10/site-packages/fiftyone/core/dataset.py:302\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, id_filepath_slice)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m d \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    301\u001b[0m     field \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m oid \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilepath\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo sample found with \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (field, id_filepath_slice)\n\u001b[1;32m    304\u001b[0m     )\n\u001b[1;32m    306\u001b[0m doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample_dict_to_doc(d)\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fos\u001b[38;5;241m.\u001b[39mSample\u001b[38;5;241m.\u001b[39mfrom_doc(doc, dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"No sample found with ID '6350559f099f10e8691afb1f'\""
     ]
    }
   ],
   "source": [
    "test_input_raw = coco_17_validation[\"6350559f099f10e8691afb1f\"]\n",
    "test_image = np.asarray(Image.open(test_input_raw.filepath)).copy()\n",
    "test_tens = tfunc.to_tensor(test_image).to(device)\n",
    "\n",
    "def draw_image(image):\n",
    "    plt.figure()\n",
    "    plt.axis(\"off\")\n",
    "    if len(image.shape) == 2:\n",
    "        plt.imshow(image, cmap=\"binary\", interpolation=\"none\")\n",
    "    else:\n",
    "        plt.imshow(image, interpolation=\"none\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "print(f\"Test image (shape={test_image.shape}):\")\n",
    "draw_image(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91edbd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ground_truth_segmentations(coco_object, draw=True):\n",
    "    image = np.asarray(Image.open(coco_object.filepath))\n",
    "    image_shape = image.shape\n",
    "    gt_segs = coco_object['segmentations']['detections']\n",
    "    segs_image = np.zeros(image_shape[:2])\n",
    "\n",
    "    for i, seg in enumerate(gt_segs, start=1):\n",
    "        mask = seg['mask']\n",
    "        label = seg['label']\n",
    "        bbox_raw = seg['bounding_box']\n",
    "        bbox_t = int(bbox_raw[1] * image_shape[0])\n",
    "        bbox_l = int(bbox_raw[0] * image_shape[1])\n",
    "        bbox_h, bbox_w = mask.shape\n",
    "        if draw:\n",
    "            print(label, bbox_h, bbox_w, mask.shape)\n",
    "        \n",
    "        segs_image[bbox_t:bbox_t+bbox_h, bbox_l:bbox_l+bbox_w] += mask*i\n",
    "    \n",
    "    if draw:\n",
    "        print(\"Segmentation:\")\n",
    "        plt.figure()\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(segs_image)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    \n",
    "    return segs_image\n",
    "    \n",
    "test_segs_image = make_ground_truth_segmentations(test_input_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8629869",
   "metadata": {},
   "source": [
    "# Get Resnet 50 and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0360b8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_backbone = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "resnet50_backbone.to(device)\n",
    "resnet50_backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c6c95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Input shape: {test_tens.shape}\")\n",
    "test_tens = test_tens.reshape((1,) + test_tens.shape)\n",
    "test_output = resnet50_backbone(test_tens)\n",
    "print(f\"Output shape: {test_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518749bb",
   "metadata": {},
   "source": [
    "# Make Odin networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c516295e",
   "metadata": {},
   "source": [
    "## Network class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d077e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FPNStage(nn.Module):\n",
    "    def __init__(self, fpn_dim, bbone_dim):\n",
    "        super().__init__()\n",
    "        self.lat = nn.Conv2d(bbone_dim, fpn_dim, kernel_size=1)\n",
    "        self.top = nn.ConvTranspose2d(fpn_dim, fpn_dim, kernel_size=4, stride=2, padding=1)\n",
    "        self.aa  = nn.Conv2d(fpn_dim, fpn_dim, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "    def forward(self, bbone_activation, prev_fpn_stage):\n",
    "        lat_out = self.lat(bbone_activation)\n",
    "        top_out = self.top(prev_fpn_stage)\n",
    "        \n",
    "        if not lat_out.shape == top_out.shape:\n",
    "            top_out = nn.UpsamplingNearest2d(size=lat_out.shape[2:])(top_out)          \n",
    "            \n",
    "        final_out = self.aa(lat_out + top_out)\n",
    "        return final_out\n",
    "        \n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, ngpu, fpn_dim=256):\n",
    "        super().__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.num_fpn_stages = 4\n",
    "        self.fpn_dim = fpn_dim\n",
    "        \n",
    "        self.activation = {}\n",
    "        self.resnet50_backbone = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        self.resnet50_backbone.layer1.register_forward_hook(self.get_activation('conv2'))\n",
    "        self.resnet50_backbone.layer2.register_forward_hook(self.get_activation('conv3'))\n",
    "        self.resnet50_backbone.layer3.register_forward_hook(self.get_activation('conv4'))\n",
    "        self.resnet50_backbone.layer4.register_forward_hook(self.get_activation('conv5'))\n",
    "        \n",
    "        self.fpn_stage_1 = nn.Conv2d(2048, self.fpn_dim, kernel_size=1)\n",
    "        self.fpn_stage_2 = FPNStage(self.fpn_dim, 1024)\n",
    "        self.fpn_stage_3 = FPNStage(self.fpn_dim, 512)\n",
    "        self.fpn_stage_4 = FPNStage(self.fpn_dim, 256)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        backbone_output = self.resnet50_backbone(input)\n",
    "        fpn_stage_1_output = self.fpn_stage_1(self.activation['conv5'])\n",
    "        fpn_stage_2_output = self.fpn_stage_2(self.activation['conv4'], fpn_stage_1_output)\n",
    "        fpn_stage_3_output = self.fpn_stage_3(self.activation['conv3'], fpn_stage_2_output)\n",
    "        fpn_stage_4_output = self.fpn_stage_4(self.activation['conv2'], fpn_stage_3_output)\n",
    "\n",
    "        \n",
    "        return fpn_stage_4_output\n",
    "        \n",
    "    def get_activation(self, name):\n",
    "        def hook(model, input, output):\n",
    "            self.activation[name] = output.detach()\n",
    "        return hook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83fae9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Projector(nn.Module):\n",
    "    def __init__(self, fpn_dim=256):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(fpn_dim, fpn_dim, 1),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Linear(fpn_dim, fpn_dim, 1),\n",
    "            nn.ReLU(inplace=False)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1accf527",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(nn.Module):\n",
    "    def __init__(self, fpn_dim=256):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(fpn_dim, fpn_dim, 1),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Linear(fpn_dim, fpn_dim, 1),\n",
    "            nn.ReLU(inplace=False)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ad19d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = FeatureExtractor(ngpu).to(device)\n",
    "g = Projector().to(device)\n",
    "q = Predictor().to(device)\n",
    "\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    f = nn.DataParallel(f, list(range(ngpu)))\n",
    "    g = nn.DataParallel(g, list(range(ngpu)))\n",
    "    q = nn.DataParallel(q, list(range(ngpu)))\n",
    "    \n",
    "    \n",
    "print(f)\n",
    "print(g)\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf852d5",
   "metadata": {},
   "source": [
    "## Test Odin networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d6684b",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = f(test_tens).transpose(1,3)\n",
    "print(h.shape)\n",
    "\n",
    "z = g(h)\n",
    "print(z.shape)\n",
    "\n",
    "p = q(z)\n",
    "print(p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56004d73",
   "metadata": {},
   "source": [
    "# Initialize and save $\\tau$, $\\theta$, and $\\xi$ parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36b6f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(f.state_dict(), \"f_tau.pth\")\n",
    "torch.save(f.state_dict(), \"f_theta.pth\")\n",
    "torch.save(f.state_dict(), \"f_xi.pth\")\n",
    "\n",
    "torch.save(g.state_dict(), \"g_tau.pth\")\n",
    "torch.save(g.state_dict(), \"g_theta.pth\")\n",
    "torch.save(g.state_dict(), \"g_xi.pth\")\n",
    "\n",
    "torch.save(q.state_dict(), \"q_tau.pth\")\n",
    "torch.save(q.state_dict(), \"q_theta.pth\")\n",
    "torch.save(q.state_dict(), \"q_xi.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9d9112",
   "metadata": {},
   "source": [
    "# View Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141cd439",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViewGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "    nn.Module sub-class which, when called on an image, generates three views of the image, v0, v1, and v2. v1\n",
    "    and v2 are generated first, and then v0 is generated from their bounding box. The class also has a\n",
    "    `reverse` method which, given an image the size of v0, produces the equivalent v1 and v2 crops from it.\n",
    "    \"\"\"\n",
    "    SCALE_RANGE = (0.08, 1.0)\n",
    "    RATIO_RANGE = (0.75, 1.33333333333)\n",
    "\n",
    "    FLIP_PROB = 0.5\n",
    "\n",
    "    COLOR_JITTER_PROB = 0.8\n",
    "    COLOR_OPERATIONS = [\"brightness\", \"contrast\", \"saturation\", \"hue\"]\n",
    "    BRIGHTNESS_MAX = 0.4\n",
    "    CONTRAST_MAX = 0.4\n",
    "    SATURATION_MAX = 0.2\n",
    "    HUE_MAX = 0.1\n",
    "\n",
    "    GRAY_PROB = 0.2\n",
    "\n",
    "    v1_BLUR_PROB = 1.0\n",
    "    v2_BLUR_PROB = 0.1\n",
    "\n",
    "    v1_SOLAR_PROB = 0.0\n",
    "    v2_SOLAR_PROB = 0.2\n",
    "\n",
    "    v0_SHAPE = (448, 448)\n",
    "    v1_SHAPE = v2_SHAPE = (224, 224)\n",
    "    INTERPOLATION = tf.InterpolationMode.BILINEAR\n",
    "\n",
    "    def __init__(self, image):\n",
    "        super().__init__()\n",
    "        self.crop_v1= tf.RandomResizedCrop.get_params(image, self.SCALE_RANGE, self.RATIO_RANGE)\n",
    "        self.crop_v2 = tf.RandomResizedCrop.get_params(image, self.SCALE_RANGE, self.RATIO_RANGE)\n",
    "        \n",
    "        flip_v1 = random.random() < self.FLIP_PROB\n",
    "        flip_v2 = random.random() < self.FLIP_PROB\n",
    "        \n",
    "        gray_v1 = random.random() < self.GRAY_PROB\n",
    "        gray_v2 = random.random() < self.GRAY_PROB\n",
    "        \n",
    "        blur_v1 = random.random() < self.v1_BLUR_PROB\n",
    "        blur_v2 = random.random() < self.v2_BLUR_PROB\n",
    "        \n",
    "        solar_v1 = random.random() < self.v1_SOLAR_PROB\n",
    "        solar_v2 = random.random() < self.v2_SOLAR_PROB\n",
    "        \n",
    "        if random.random() < self.COLOR_JITTER_PROB: \n",
    "            color_params = tf.ColorJitter.get_params(\n",
    "                [max(0, 1 - self.BRIGHTNESS_MAX), 1 + self.BRIGHTNESS_MAX],\n",
    "                [max(0, 1 - self.CONTRAST_MAX), 1 + self.CONTRAST_MAX],\n",
    "                [max(0, 1 - self.SATURATION_MAX), 1 + self.SATURATION_MAX],\n",
    "                [-self.HUE_MAX, self.HUE_MAX]\n",
    "            )\n",
    "            order = color_params[0]\n",
    "            color_params = color_params[1:]\n",
    "            jitter_v1 = [(self.COLOR_OPERATIONS[i], color_params[i]) for i in order]\n",
    "        else:\n",
    "            jitter_v1 = None\n",
    "        if random.random() < self.COLOR_JITTER_PROB: \n",
    "            color_params = tf.ColorJitter.get_params(\n",
    "                [max(0, 1 - self.BRIGHTNESS_MAX), 1 + self.BRIGHTNESS_MAX],\n",
    "                [max(0, 1 - self.CONTRAST_MAX), 1 + self.CONTRAST_MAX],\n",
    "                [max(0, 1 - self.SATURATION_MAX), 1 + self.SATURATION_MAX],\n",
    "                [-self.HUE_MAX, self.HUE_MAX]\n",
    "            )\n",
    "            order = color_params[0]\n",
    "            color_params = color_params[1:]\n",
    "            jitter_v2 = [(self.COLOR_OPERATIONS[i], color_params[i]) for i in order]\n",
    "        else:\n",
    "            jitter_v2 = None\n",
    "        \n",
    "        self.v1_params = (self.crop_v1, flip_v1, jitter_v1, gray_v1, blur_v1, solar_v1, self.v1_SHAPE)\n",
    "        self.v2_params = (self.crop_v2, flip_v2, jitter_v2, gray_v2, blur_v2, solar_v2, self.v2_SHAPE)\n",
    "        \n",
    "        self.v1_proportional_crop = None\n",
    "        self.v2_proportional_crop = None\n",
    "        \n",
    "    def __call__(self, img):\n",
    "        v1 = self._generate_sub_view(img, self.v1_params)\n",
    "        v2 = self._generate_sub_view(img, self.v2_params)\n",
    "        v0 = self._generate_whole_view(img, self.crop_v1, self.crop_v2)\n",
    "        \n",
    "        return v0, v1, v2\n",
    "    \n",
    "    def reverse(self, image):\n",
    "        image_height, image_width = image.shape[-2:]\n",
    "        \n",
    "        v1_top_scaled = round(self.v1_proportional_crop[0] * image_height)\n",
    "        v1_left_scaled = round(self.v1_proportional_crop[1] * image_width)\n",
    "        v1_height_scaled = round(self.v1_proportional_crop[2] * image_height)\n",
    "        v1_width_scaled = round(self.v1_proportional_crop[3] * image_width)\n",
    "        \n",
    "        v2_top_scaled = round(self.v2_proportional_crop[0] * image_height)\n",
    "        v2_left_scaled = round(self.v2_proportional_crop[1] * image_width)\n",
    "        v2_height_scaled = round(self.v2_proportional_crop[2] * image_height)\n",
    "        v2_width_scaled = round(self.v2_proportional_crop[3] * image_width)\n",
    "        \n",
    "        v1_scaled = tfunc.resized_crop(image, v1_top_scaled, v1_left_scaled, v1_height_scaled, v1_width_scaled, self.v1_SHAPE, tf.InterpolationMode.BILINEAR)\n",
    "        v2_scaled = tfunc.resized_crop(image, v2_top_scaled, v2_left_scaled, v2_height_scaled, v2_width_scaled, self.v2_SHAPE, tf.InterpolationMode.BILINEAR)\n",
    "        \n",
    "        if self.v1_params[1]: v1_scaled = tfunc.hflip(v1_scaled)\n",
    "        if self.v2_params[1]: v2_scaled = tfunc.hflip(v2_scaled)\n",
    "        \n",
    "        return v1_scaled, v2_scaled\n",
    "        \n",
    "    def _generate_whole_view(self, image, crop_v1, crop_v2):\n",
    "        v1_top, v1_left, v1_height, v1_width = crop_v1\n",
    "        v2_top, v2_left, v2_height, v2_width = crop_v2\n",
    "        v1_bot = v1_top + v1_height\n",
    "        v1_right = v1_left + v1_width\n",
    "        v2_bot = v2_top + v2_height\n",
    "        v2_right = v2_left + v2_width\n",
    "        \n",
    "        v0_top = min(v1_top, v2_top)\n",
    "        v0_left = min(v1_left, v2_left)\n",
    "        v0_bot = max(v1_bot, v2_bot)    \n",
    "        v0_right = max(v1_right, v2_right)\n",
    "        v0_height = v0_bot - v0_top\n",
    "        v0_width = v0_right - v0_left\n",
    "        \n",
    "        self.v0_crop = (v0_top, v0_left, v0_height, v0_width)\n",
    "        \n",
    "        v1_proportional_top = (v1_top - v0_top)/v0_height\n",
    "        v1_proportional_left = (v1_left - v0_left)/v0_width\n",
    "        v1_proportional_height = v1_height/v0_height\n",
    "        v1_proportional_width = v1_width/v0_width\n",
    "        self.v1_proportional_crop = (v1_proportional_top, v1_proportional_left, v1_proportional_height, v1_proportional_width)\n",
    "        \n",
    "        v2_proportional_top = (v2_top - v0_top)/v0_height\n",
    "        v2_proportional_left = (v2_left - v0_left)/v0_width\n",
    "        v2_proportional_height = v2_height/v0_height\n",
    "        v2_proportional_width = v2_width/v0_width\n",
    "        self.v2_proportional_crop = (v2_proportional_top, v2_proportional_left, v2_proportional_height, v2_proportional_width)\n",
    "        \n",
    "        return tfunc.resized_crop(image, v0_top, v0_left, v0_height, v0_width, self.v0_SHAPE, tf.InterpolationMode.BILINEAR)\n",
    "    \n",
    "    def _generate_sub_view(self, image, params):\n",
    "        crop, flip, jitter, gray, blur, solar, shape = params\n",
    "        \n",
    "        t, l, h, w = crop\n",
    "        output = tfunc.resized_crop(image, t, l, h, w, shape, self.INTERPOLATION)\n",
    "        if flip: output = tfunc.hflip(output)\n",
    "        if jitter is not None:\n",
    "            for param, value in jitter:\n",
    "                if param == \"brightness\": output = tfunc.adjust_brightness(output, value)\n",
    "                elif param == \"contrast\": output = tfunc.adjust_contrast(output, value)\n",
    "                elif param == \"hue\": output = tfunc.adjust_hue(output, value)\n",
    "                elif param == \"saturation\": output = tfunc.adjust_saturation(output, value)\n",
    "        if gray: output = tfunc.rgb_to_grayscale(output, 3)\n",
    "        if blur: output = tfunc.gaussian_blur(output, 23, (0.1, 2.0))\n",
    "        if solar: output = tfunc.solarize(output, 0.5)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edccdf1",
   "metadata": {},
   "source": [
    "## Test View Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9664453",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "view_gen = ViewGenerator(test_tens)\n",
    "\n",
    "v0, v1, v2 = view_gen(test_tens)\n",
    "\n",
    "print(f\"Inputted test image ({test_image.shape}):\")\n",
    "draw_image(test_image)\n",
    "\n",
    "print(f\"v0 ({v0.shape}):\")\n",
    "draw_image(v0.squeeze().detach().cpu().numpy().transpose(1,2,0))\n",
    "print(f\"v1 ({v1.shape}):\")\n",
    "draw_image(v1.squeeze().detach().cpu().numpy().transpose(1,2,0))\n",
    "print(f\"v2 ({v2.shape}):\")\n",
    "draw_image(v2.squeeze().detach().cpu().numpy().transpose(1,2,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e37d68",
   "metadata": {},
   "source": [
    "## Test apply_transforms to recover alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4406b67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_gt_segs = make_ground_truth_segmentations(test_input_raw, draw=False)\n",
    "v0_segs = tfunc.resized_crop(tfunc.to_tensor(test_image_gt_segs), *view_gen.v0_crop, (448, 448), tf.InterpolationMode.BILINEAR)\n",
    "v1_segs, v2_segs = view_gen.reverse(v0_segs)\n",
    "\n",
    "print(f\"v0 segmentation shape: {v0_segs.shape}\")\n",
    "draw_image(v0_segs.squeeze())\n",
    "print(f\"v1 segmentation shape: {v1_segs.shape}\")\n",
    "draw_image(v1_segs.squeeze())\n",
    "print(f\"v2 segmentation shape: {v2_segs.shape}\")\n",
    "draw_image(v2_segs.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c916f521",
   "metadata": {},
   "source": [
    "# Apply $\\tau$ Odin networks on generated v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1e2000",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f.load_state_dict(torch.load(\"f_tau.pth\"))\n",
    "g.load_state_dict(torch.load(\"g_tau.pth\"))\n",
    "q.load_state_dict(torch.load(\"q_tau.pth\"))\n",
    "\n",
    "h0 = f(v0)\n",
    "h0 = tfunc.resize(h0, (448, 448)).transpose(1,3)\n",
    "print(h0.shape)\n",
    "\n",
    "z0 = g(h0)\n",
    "print(z0.shape)\n",
    "\n",
    "p0 = q(z0)\n",
    "print(p0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b8ec21",
   "metadata": {},
   "source": [
    "# Apply K-means to h0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e4aac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "h0_np = np.transpose(h0.detach().cpu().squeeze(),(1,2,0))\n",
    "\n",
    "original_shape = h0_np.shape\n",
    "flat_shape = (original_shape[0]*original_shape[1], original_shape[2])\n",
    "\n",
    "h0_np_flat = h0_np.reshape((flat_shape))\n",
    "print(f\"Original shape: {h0_np.shape}\")\n",
    "print(f\"Flattened shape: {h0_np_flat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8eade8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = KMeans(n_clusters=8, init='k-means++', n_init=10, max_iter=300, tol=0.0001, verbose=0, random_state=None, copy_x=True, algorithm='lloyd')\n",
    "# clusterer = OPTICS(cluster_method=\"xi\", xi=0.01, n_jobs=4)\n",
    "mask_assignments_flat = clusterer.fit_predict(h0_np_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c845a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_ids = set(mask_assignments_flat)\n",
    "num_masks = len(cluster_ids)\n",
    "print(f\"Num masks: {num_masks}\")\n",
    "print(cluster_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986a59cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_assignments = tfunc.resize(tfunc.to_tensor(mask_assignments_flat.reshape(original_shape[:2])), (448, 448), interpolation=tf.InterpolationMode.NEAREST).squeeze()\n",
    "print(\"v0:\")\n",
    "draw_image(v0.detach().cpu().squeeze().numpy().transpose(1,2,0))\n",
    "print(\"Mask assignments on v0\")\n",
    "draw_image(mask_assignments.squeeze())\n",
    "print(\"v1:\")\n",
    "draw_image(v1.detach().cpu().squeeze().numpy().transpose(1,2,0))\n",
    "print(\"v2:\")\n",
    "draw_image(v2.detach().cpu().squeeze().numpy().transpose(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5c662e",
   "metadata": {},
   "outputs": [],
   "source": [
    "m0_layers = [torch.where(mask_assignments==c_id, 1., 0.).numpy() for c_id in range(num_masks)]\n",
    "m0_np = np.stack(m0_layers, 2)\n",
    "m0 = tfunc.to_tensor(m0_np)\n",
    "print(m0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fc4a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1_raw, m2_raw = view_gen.reverse(m0)\n",
    "m1_raw = m1_raw > 0\n",
    "m1_raw = m1_raw.to(torch.float32)\n",
    "m1_nonempty_idxs = []\n",
    "for idx, mask in enumerate(m1_raw):\n",
    "    if mask.sum() != 0:\n",
    "        m1_nonempty_idxs.append(idx)\n",
    "        \n",
    "if len(m1_nonempty_idxs) != 0:\n",
    "    m1 = tfunc.to_tensor(np.stack([m1_raw[i] for i in m1_nonempty_idxs], 2)).to(device)\n",
    "    m1 = m1.reshape((1,)+m1.shape)\n",
    "    print(f\"m1 shape: {m1.shape}\")\n",
    "\n",
    "    m1_np = m1.detach().cpu().numpy().squeeze()\n",
    "    draw_image(np.transpose(vutils.make_grid([tfunc.to_tensor(m1_i_np) for m1_i_np in m1_np], padding=5, normalize=True, pad_value = 0.5), (1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038e614d",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2_raw = m2_raw > 0\n",
    "m2_raw = m2_raw.to(torch.float32)\n",
    "m2_nonempty_idxs = []\n",
    "for idx, mask in enumerate(m2_raw):\n",
    "    if mask.sum() != 0:\n",
    "        m2_nonempty_idxs.append(idx)\n",
    "        \n",
    "if len(m2_nonempty_idxs) != 0:\n",
    "    m2 = tfunc.to_tensor(np.stack([m2_raw[i] for i in m2_nonempty_idxs], 2)).to(device)\n",
    "    m2 = m2.reshape((1,)+m2.shape)\n",
    "    print(f\"m2 shape: {m2.shape}\")\n",
    "\n",
    "m2_np = m2.detach().cpu().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde8b947",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_image(m0_np[:,:,5])\n",
    "draw_image(m1_np[5])\n",
    "draw_image(m2_np[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6c27a8",
   "metadata": {},
   "source": [
    "# Apply $\\theta$ networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6264ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.load_state_dict(torch.load(\"f_theta.pth\"))\n",
    "g.load_state_dict(torch.load(\"g_theta.pth\"))\n",
    "q.load_state_dict(torch.load(\"q_theta.pth\"))\n",
    "\n",
    "h_1_theta = tfunc.resize(f(v1), (224, 224), tf.InterpolationMode.BILINEAR)\n",
    "print(h_1_theta.shape)\n",
    "\n",
    "h_2_theta = tfunc.resize(f(v2), (224, 224), tf.InterpolationMode.BILINEAR)\n",
    "print(h_2_theta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6004f5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_k_1_thetas = torch.concat([(1/(m1_i.sum()))*(torch.where(m1_i==1, h_1_theta, 0).sum(dim=[2,3])) for m1_i in m1[0]])\n",
    "h_k_2_thetas = torch.concat([(1/(m2_i.sum()))*(torch.where(m2_i==1, h_2_theta, 0).sum(dim=[2,3])) for m2_i in m2[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be829099",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"h_k_1_thetas shape: {h_k_1_thetas.shape}, device: {h_k_1_thetas.device}\")\n",
    "print(f\"h_k_2_thetas shape: {h_k_2_thetas.shape}, device: {h_k_2_thetas.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ce1a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_k_1_theta = g(h_k_1_thetas)\n",
    "print(f\"z_k_1_theta shape: {z_k_1_theta.shape}, device: {z_k_1_theta.device}\")\n",
    "z_k_2_theta = g(h_k_2_thetas)\n",
    "print(f\"z_k_2_theta shape: {z_k_2_theta.shape}, device: {z_k_2_theta.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76b8b57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p_k_1_theta = q(z_k_1_theta)\n",
    "print(f\"p_k_1_theta shape: {p_k_1_theta.shape}, device: {p_k_1_theta.device}\")\n",
    "p_k_2_theta = q(z_k_2_theta)\n",
    "print(f\"p_k_2_theta shape: {p_k_2_theta.shape}, device: {p_k_2_theta.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418c36f1",
   "metadata": {},
   "source": [
    "# Apply $\\xi$ networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694b28d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.load_state_dict(torch.load(\"f_xi.pth\"))\n",
    "g.load_state_dict(torch.load(\"g_xi.pth\"))\n",
    "q.load_state_dict(torch.load(\"q_xi.pth\"))\n",
    "\n",
    "h_1_xi = tfunc.resize(f(v1), (224, 224), tf.InterpolationMode.BILINEAR)\n",
    "h_2_xi = tfunc.resize(f(v2), (224, 224), tf.InterpolationMode.BILINEAR)\n",
    "print(h_1_xi.shape)\n",
    "print(h_2_xi.shape)\n",
    "\n",
    "h_k_1_xi = torch.concat([(1/(m1_i.sum()))*(torch.where(m1_i==1, h_1_xi, 0).sum(dim=[2,3])) for m1_i in m1[0]])\n",
    "h_k_2_xi = torch.concat([(1/(m2_i.sum()))*(torch.where(m2_i==1, h_2_xi, 0).sum(dim=[2,3])) for m2_i in m2[0]])\n",
    "print(f\"h_k_1_xi shape: {h_k_1_xi.shape}, device: {h_k_1_xi.device}\")\n",
    "print(f\"h_k_2_xi shape: {h_k_2_xi.shape}, device: {h_k_2_xi.device}\")\n",
    "\n",
    "z_k_1_xi = g(h_k_1_xi)\n",
    "z_k_2_xi = g(h_k_2_xi)\n",
    "print(f\"z_k_1_xi shape: {z_k_1_xi.shape}, device: {z_k_1_xi.device}\")\n",
    "print(f\"z_k_2_xi shape: {z_k_2_xi.shape}, device: {z_k_2_xi.device}\")\n",
    "\n",
    "p_k_1_xi = q(z_k_1_xi)\n",
    "p_k_2_xi = q(z_k_2_xi)\n",
    "print(f\"p_k_1_xi shape: {p_k_1_xi.shape}, device: {p_k_1_xi.device}\")\n",
    "print(f\"z_k_2_xi shape: {p_k_2_xi.shape}, device: {p_k_2_xi.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003e0151",
   "metadata": {},
   "source": [
    "# Compute loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b932a1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_mask_similarity(pk1_theta, zk2_xi, alpha=0.1):\n",
    "    top = torch.dot(pk1_theta, zk2_xi)\n",
    "    bot = torch.norm(pk1_theta) * torch.norm(zk2_xi)\n",
    "    return (1/alpha)*(top/bot)\n",
    "\n",
    "def feature_contrastive_loss(p_k_1_theta, z_k_2_xi, index, alpha=0.1):\n",
    "    positive_similarity = single_mask_similarity(p_k_1_theta[index], z_k_2_xi[index], alpha)\n",
    "    negative_similarities_sum = sum([single_mask_similarity(p_k_1_theta[index], zk2_xi, alpha) for i, zk2_xi in enumerate(z_k_2_xi) if i != index])\n",
    "    bot = positive_similarity + negative_similarities_sum\n",
    "    nll = -1*torch.log(positive_similarity/bot)\n",
    "    return nll\n",
    "\n",
    "def total_contrastive_loss(p_k_1_theta, p_k_2_theta, z_k_1_xi, z_k_2_xi, alpha):\n",
    "    cum_loss = torch.Tensor([0.]).to(device)\n",
    "    for mask_idx in range(num_masks):\n",
    "        l_12_k = feature_contrastive_loss(p_k_1_theta, z_k_2_xi, mask_idx, alpha)\n",
    "        l_21_k = feature_contrastive_loss(p_k_2_theta, z_k_1_xi, mask_idx, alpha)\n",
    "        cum_loss += l_12_k + l_21_k\n",
    "    return cum_loss/num_masks\n",
    "        \n",
    "\n",
    "loss = total_contrastive_loss(p_k_1_theta, p_k_2_theta, z_k_1_xi, z_k_2_xi, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9093c40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0685e5e3",
   "metadata": {},
   "source": [
    "# Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7e17d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.load_state_dict(torch.load(\"f_theta.pth\"))\n",
    "g.load_state_dict(torch.load(\"g_theta.pth\"))\n",
    "q.load_state_dict(torch.load(\"q_theta.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98b9510",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "optimizer = optim.SGD(q.parameters(), lr=0.05)\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc914494",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c9b928",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "torchviz.make_dot(loss, params=dict(q.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1511008",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_test = h0 - 0\n",
    "loss_test = loss_test.sum()\n",
    "print(loss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05ee213",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "torchviz.make_dot(loss_test, params=dict(q.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68f0309",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "optimizer = optim.SGD(q.parameters(), lr=0.05)\n",
    "optimizer.zero_grad()\n",
    "loss_test.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97d1021",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
